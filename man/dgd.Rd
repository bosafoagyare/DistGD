% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dgd.R
\name{dgd}
\alias{dgd}
\title{Distributed Gradient Descent}
\usage{
dgd(
  sc,
  f_list,
  grad_list = NULL,
  init_xs,
  init_step_size,
  weight_mat,
  num_iters,
  print = FALSE,
  make_trace = FALSE
)
}
\arguments{
\item{sc}{a connection to a Spark cluster.}

\item{f_list}{a list of local objective functions.}

\item{grad_list}{an optional list of the gradients of the functions in
\code{f_list}. Must be written as R functions. If not supplied,
\code{\link[numDeriv]{grad()}} in the \code{numDeriv} package is used to approximate them.}

\item{init_xs}{a list of initial values.}

\item{init_step_size}{An initial step size.}

\item{weight_mat}{a matrix of weights of the connections between the agents.}

\item{num_iters}{the number of iterations to perform.}

\item{print}{a logical value indicating whether to print the current estimates on each iteration.}

\item{make_trace}{a logical value indicating whether to return a list with the minimizer estimates from
the iterations.}
}
\value{
a list of global min/max from each network
}
\description{
\code{dgd} optimizes a global objective function  expressed as a
sum of a list of local objective functions belonging to different agents
situated in a network. It returns unified list of the optimal values from
each agent.
}
